{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning with Unsloth\n",
    "\n",
    "This notebook demonstrates how to fine-tune a language model using Unsloth, a library that makes fine-tuning LLMs faster and more memory-efficient.\n",
    "\n",
    "## What is Unsloth?\n",
    "\n",
    "Unsloth optimizes the fine-tuning process for large language models, providing:\n",
    "- Up to 2x faster fine-tuning\n",
    "- Lower memory usage (fits larger batch sizes)\n",
    "- Support for various models (Llama, Mistral, Gemma, Phi, etc.)\n",
    "- Easy integration with Hugging Face's ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "If you haven't installed Unsloth yet, uncomment and run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install Unsloth\n",
    "# !pip install \"unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up the configuration parameters for fine-tuning. Feel free to modify these based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"unsloth/llama-3-8b-bnb-4bit\"  # Pre-quantized model for faster loading\n",
    "MAX_SEQ_LENGTH = 2048  # Context length\n",
    "LOAD_IN_4BIT = True    # Use 4-bit quantization to reduce memory usage\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_RANK = 16         # Rank of LoRA matrices (higher = more capacity, more memory)\n",
    "LORA_ALPHA = 16        # LoRA alpha parameter\n",
    "LORA_DROPOUT = 0       # LoRA dropout (0 is optimized)\n",
    "\n",
    "# Training configuration\n",
    "BATCH_SIZE = 1         # Batch size per device\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Accumulate gradients over multiple steps\n",
    "LEARNING_RATE = 2e-4   # Learning rate\n",
    "MAX_STEPS = 100        # Number of training steps (set to None to train for full epochs)\n",
    "NUM_EPOCHS = None      # Number of epochs (alternative to max_steps)\n",
    "OUTPUT_DIR = \"outputs\" # Directory to save the model\n",
    "\n",
    "# Prompt template for instruction fine-tuning\n",
    "PROMPT_TEMPLATE = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{response}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model and Tokenizer\n",
    "\n",
    "Load the pre-trained model and tokenizer using Unsloth's optimized loading function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Loading model and tokenizer...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect dtype\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    "    # token=\"hf_...\",  # Uncomment and add your token if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply LoRA for Efficient Fine-Tuning\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a technique that allows efficient fine-tuning by adding small, trainable matrices to the model's weights instead of updating all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Applying LoRA for efficient fine-tuning...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",  # \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # \"unsloth\" uses 30% less VRAM\n",
    "    random_state=42,\n",
    "    use_rslora=False,  # Rank-stabilized LoRA (optional)\n",
    "    loftq_config=None, # LoftQ (optional)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Prepare Dataset\n",
    "\n",
    "Load a dataset and format it for training. This example uses the Alpaca dataset, but you can replace it with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Loading dataset...\")\n",
    "# Option 1: Load a dataset from Hugging Face\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "\n",
    "# Option 2: Load your own dataset (uncomment to use)\n",
    "# dataset = load_dataset(\"json\", data_files=\"your_data.json\", split=\"train\")\n",
    "\n",
    "# Display a sample from the dataset\n",
    "print(\"\\nSample from the dataset:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to format the prompts\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    responses = examples[\"output\"]\n",
    "    \n",
    "    texts = []\n",
    "    for instruction, response in zip(instructions, responses):\n",
    "        # Format the text according to the template and add EOS token\n",
    "        text = PROMPT_TEMPLATE.format(instruction=instruction, response=response) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "print(\"Formatting dataset...\")\n",
    "formatted_dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Display a formatted example\n",
    "print(\"\\nFormatted example:\")\n",
    "print(formatted_dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Trainer\n",
    "\n",
    "Configure the SFTTrainer with the model, tokenizer, and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Setting up trainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=2,  # Number of processes for dataset preparation\n",
    "    packing=False,  # Set to True for short sequences to speed up training\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=5,\n",
    "        max_steps=MAX_STEPS,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "Start the training process and monitor GPU memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Starting training...\")\n",
    "# Print GPU information\n",
    "if torch.cuda.is_available():\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {gpu_stats.name}\")\n",
    "    print(f\"Total GPU memory: {round(gpu_stats.total_memory / 1024 / 1024 / 1024, 2)} GB\")\n",
    "    \n",
    "# Track memory usage\n",
    "start_gpu_memory = 0\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"Starting GPU memory usage: {start_gpu_memory} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train the model\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Print training statistics\n",
    "print(\"\\n===== TRAINING COMPLETE =====\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training time: {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes\")\n",
    "\n",
    "# Print memory usage\n",
    "if torch.cuda.is_available():\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
    "    print(f\"Peak memory usage: {used_memory} GB\")\n",
    "    print(f\"Memory used for training: {used_memory_for_training} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Model\n",
    "\n",
    "Save the fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\nSaving model...\")\n",
    "# Save the LoRA adapter only (small file)\n",
    "model.save_pretrained(f\"{OUTPUT_DIR}/lora_adapter\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/lora_adapter\")\n",
    "print(f\"LoRA adapter saved to {OUTPUT_DIR}/lora_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Optional: Save the merged model (much larger file)\n",
    "print(\"\\nSaving merged model (this may take a while)...\")\n",
    "model.save_pretrained_merged(f\"{OUTPUT_DIR}/merged_model\", tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"Merged model saved to {OUTPUT_DIR}/merged_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Model\n",
    "\n",
    "Test the fine-tuned model with some example prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clean up CUDA memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(instruction, max_new_tokens=512):\n",
    "    # Format the prompt\n",
    "    prompt = PROMPT_TEMPLATE.format(instruction=instruction, response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the response part\n",
    "    response_start = generated_text.find(\"### Response:\")\n",
    "    if response_start != -1:\n",
    "        response = generated_text[response_start + len(\"### Response:\"):].strip()\n",
    "    else:\n",
    "        response = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test with an example\n",
    "test_instruction = \"Explain the concept of fine-tuning in machine learning.\"\n",
    "print(f\"Instruction: {test_instruction}\")\n",
    "print(\"\\nResponse:\")\n",
    "print(generate_text(test_instruction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Convert to GGUF Format (Optional)\n",
    "\n",
    "Convert the model to GGUF format for use with llama.cpp or Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Uncomment to convert to GGUF format\n",
    "# !python -m unsloth.convert_to_gguf outputs/merged_model --outfile my_model.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Use with Ollama (Optional)\n",
    "\n",
    "Create a Modelfile for use with Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a Modelfile\n",
    "modelfile_content = \"\"\"FROM my_model.gguf\n",
    "SYSTEM You are a helpful assistant that provides accurate and informative responses.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"Modelfile\", \"w\") as f:\n",
    "    f.write(modelfile_content)\n",
    "\n",
    "print(\"Modelfile created. To use with Ollama:\")\n",
    "print(\"1. Install Ollama from https://ollama.com\")\n",
    "print(\"2. Run: ollama create mymodel -f Modelfile\")\n",
    "print(\"3. Run: ollama run mymodel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}